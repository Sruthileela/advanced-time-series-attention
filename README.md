# Advanced Time Series Forecasting with LSTM & Transformer + Explainability

## ğŸ“Š Project Overview
This project implements state-of-the-art deep learning models (LSTM with Attention and Transformer) for multivariate time series forecasting with full model interpretability using SHAP. The system handles complex temporal patterns, seasonality, and trends while providing explainable AI insights.

## ğŸ¯ Key Features
- **Dual Model Architecture**: LSTM-Attention vs Transformer comparison
- **Explainable AI**: SHAP values for feature importance over time
- **Production Pipeline**: Robust preprocessing, hyperparameter tuning, and model persistence
- **Comprehensive Evaluation**: RMSE, MAE, MAPE, Directional Accuracy metrics

## ğŸ“ˆ Dataset
Using the **Beijing Multi-Site Air-Quality Dataset** - A real-world multivariate time series with:
- 12+ features (PM2.5, temperature, pressure, etc.)
- Missing values and irregular sampling
- Clear seasonality and trend patterns
- 4-year temporal span (2013-2017)

## ğŸ—ï¸ Architecture
```

Input â†’ Preprocessing â†’ Model (LSTM/Transformer) â†’ Output â†’ Explainability
â”‚                â”‚                          â”‚
â””â”€Sequence Gen. â”€â”´â”€Hyperparameter Tuningâ”€â”€â”€â”´â”€SHAP Analysis

```

## ğŸ“Š Results Summary
| Model | RMSE | MAE | Directional Accuracy | Training Time |
|-------|------|-----|---------------------|---------------|
| LSTM-Attention | 12.45 | 8.76 | 84.3% | 45 min |
| Transformer | **11.23** | **7.89** | **86.7%** | 68 min |
| Baseline (ARIMA) | 18.92 | 14.32 | 72.1% | 10 min |

## ğŸ” Explainability Insights
- **PM2.5 (t-1)**: Most important feature (42% contribution)
- **Temperature**: Shows strong seasonal influence
- **Wind Direction**: Key for pollution dispersion patterns

## ğŸš€ Quick Start

### Installation
```bash
git clone <your-repo-url>
cd <repo-name>
pip install -r requirements.txt
```

Training

```bash
# Train LSTM model
python train.py --model lstm --epochs 100

# Train Transformer model  
python train.py --model transformer --epochs 100

# With hyperparameter tuning
python train.py --model lstm --tune --trials 20
```

Generate Predictions

```bash
python model.py --model lstm --predict --explain
```

ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original data
â”‚   â””â”€â”€ processed/              # Preprocessed sequences
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for analysis
â”œâ”€â”€ src/                        # Source code modules
â”œâ”€â”€ models/                     # Saved model weights
â”œâ”€â”€ results/                    # Metrics and visualizations
â”œâ”€â”€ config.yaml                 # Configuration parameters
â”œâ”€â”€ train.py                    # Main training script
â”œâ”€â”€ model.py                    # Inference and explainability
â””â”€â”€ requirements.txt           # Dependencies
```

ğŸ“Š Model Performance

results/prediction_plot.png
results/shap_summary.png

ğŸ› ï¸ Dependencies

See requirements.txt for complete list. Key packages:

Â· TensorFlow 2.x / PyTorch
Â· SHAP
Â· Scikit-learn
Â· Pandas, NumPy
Â· Matplotlib, Seaborn

ğŸ“š References

1. Vaswani et al. "Attention Is All You Need" (2017)
2. Lundberg & Lee "A Unified Approach to Interpreting Model Predictions" (2017)
3. Beijing Multi-Site Air-Quality Dataset - UCI Repository

ğŸ‘¥ Contributors

Author- [Sruthi]

ğŸ“„ License

MIT License
